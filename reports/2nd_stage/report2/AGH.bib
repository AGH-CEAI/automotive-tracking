@article{ByteTrack,
  title={ByteTrack: Multi-Object Tracking by Associating Every Detection Box},
  author={Zhang, Yifu and Sun, Peize and Jiang, Yi and Yu, Dongdong and Weng, Fucheng and Yuan, Zehuan and Luo, Ping and Liu, Wenyu and Wang, Xinggang},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2022}
}
@article{Bayesian_online_tracking,
author = {Jorge, Pedro and Abrantes, Arnaldo and Marques, Jorge},
year = {2004},
month = {01},
pages = {},
title = {ON-LINE OBJECT TRACKING WITH BAYESIAN NETWORKS}
}
@article{Bayesian_tracking,
title = {A multi-object tracker using dynamic Bayesian networks and a residual neural network based similarity estimator},
journal = {Computer Vision and Image Understanding},
volume = {225},
pages = {103569},
year = {2022},
issn = {1077-3142},
doi = {https://doi.org/10.1016/j.cviu.2022.103569},
url = {https://www.sciencedirect.com/science/article/pii/S1077314222001473},
author = {Mohamad Saada and Christos Kouppas and Baihua Li and Qinggang Meng},
keywords = {Multi-object tracking, Dynamic Bayesian networks, Residual neural networks, YOLO V5, MOTChallenge},
abstract = {In this paper we introduce a novel multi-object tracker based on the tracking-by-detection paradigm. This tracker utilises a Dynamic Bayesian Network for predicting objects’ positions through filtering and updating in real-time. The algorithm is trained and then tested using the MOTChallenge11https://motchallenge.net/. challenge benchmark of video sequences. After initial testing, a state-of-the-art residual neural network for extracting feature descriptors is used. This ResNet feature extractor is integrated into the tracking algorithm for object similarity estimation to further enhance tracker performance. Finally, we demonstrate the effects of object detection on tracker performance using a custom trained state of the art You Only Look Once (YOLO) V5 object detector. Results are analysed and evaluated using the MOTChallenge Evaluation Kit, followed by a comparison to state-of-the-art methods.}
}
@inproceedings{CRF_human_tracking,
	address = {Barcelona, Spain},
	title = {Detection-based multi-human tracking using a {CRF} model},
	isbn = {978-1-4673-0063-6 978-1-4673-0062-9 978-1-4673-0061-2},
	url = {http://ieeexplore.ieee.org/document/6130451/},
	doi = {10.1109/ICCVW.2011.6130451},
	abstract = {In surveillance videos, the task of tracking multiple people is of primary importance and is often a preliminary step before applying higher-level algorithms, e.g. to analyze interactions or to recognize behaviors. In this paper, we take a tracking-by-detection approach and formulate multi-person tracking as a statistical data association problem which seeks for the optimal label ﬁeld in which detections belonging to the same person have the same label. Speciﬁcally, unlike most previous works that rely on generative approaches, we use a Conditional Random Field (CRF) model, whose pairwise detection factors, deﬁned for both distance and color features, are modeled using a twohypothesis framework: a pair of detections corresponds either to the same person or not. Parameters of these twohypothesis model factors are learned in a fully unsupervised way from data. Optimization is conducted using a deterministic sliding window method. Qualitative and quantitative results on several different surveillance datasets show that our method can generate robust and accurate tracks in spite of the noisy output of the human detector and of occlusions.},
	language = {en},
	urldate = {2024-05-29},
	booktitle = {2011 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops} ({ICCV} {Workshops})},
	publisher = {IEEE},
	author = {Heili, Alexandre and Chen, Cheng and Odobez, Jean-Marc},
	month = nov,
	year = {2011},
	pages = {1673--1680},
	file = {Heili et al. - 2011 - Detection-based multi-human tracking using a CRF m.pdf:C\:\\Users\\kakic\\Zotero\\storage\\977T9IHT\\Heili et al. - 2011 - Detection-based multi-human tracking using a CRF m.pdf:application/pdf},
}
@ARTICLE{CRF_object_tracking,
  author={Xiang, Jun and Xu, Guohan and Ma, Chao and Hou, Jianhua},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={End-to-End Learning Deep CRF Models for Multi-Object Tracking Deep CRF Models}, 
  year={2021},
  volume={31},
  number={1},
  pages={275-288},
  keywords={Target tracking;Machine learning;Recurrent neural networks;Optimization;Task analysis;Standards;Inference algorithms;Multi-object tracking;end-to-end deep learning;conditional random field;data association},
  doi={10.1109/TCSVT.2020.2975842}}
@article{object_detection_with_SVM,
doi = {10.1088/1755-1315/18/1/012014},
url = {https://dx.doi.org/10.1088/1755-1315/18/1/012014},
year = {2014},
month = {feb},
publisher = {},
volume = {18},
number = {1},
pages = {012014},
author = {P D Wardaya},
title = {Support vector machine as a binary classifier for automated object detection in remotely sensed data},
journal = {IOP Conference Series: Earth and Environmental Science},
abstract = {In the present paper, author proposes the application of Support Vector Machine (SVM) for the analysis of satellite imagery. One of the advantages of SVM is that, with limited training data, it may generate comparable or even better results than the other methods. The SVM algorithm is used for automated object detection and characterization. Specifically, the SVM is applied in its basic nature as a binary classifier where it classifies two classes namely, object and background. The algorithm aims at effectively detecting an object from its background with the minimum training data. The synthetic image containing noises is used for algorithm testing. Furthermore, it is implemented to perform remote sensing image analysis such as identification of Island vegetation, water body, and oil spill from the satellite imagery. It is indicated that SVM provides the fast and accurate analysis with the acceptable result.}
}
@article{object_recognition_with_SVM,
title = {Learning Filters for Object Recognition with Linear Support Vector Machine},
journal = {Procedia Engineering},
volume = {15},
pages = {1657-1661},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.309},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811018108},
author = {Jiabao Wang and Jianjiang Lu and Yafei Zhang and Weiguang Xu},
keywords = {Object recognition, Partial gradient descent, Linear SVM},
abstract = {This paper is contributed to object recognition with linear Support Vector Machine (SVM), from which we can learn the filters for recognizing the particular object categories. We compare two different SVM solvers for the primal and dual problems, and present a new fast training method, called as Partial Gradient Descent (PGD), to estimate the filters from the large-scale dataset. Our learning approach is directly proposed for the primal problem not for the dual, but it has great generalization performance and training efficiency than the popular dual optimization methods when we just want to find an approximate solution. Experiment proves the success of our learning method, especially in the large-scale truth dataset.}
}
@article{SVM_QUBO_dWave,
title = {Support vector machines on the D-Wave quantum annealer},
journal = {Computer Physics Communications},
volume = {248},
pages = {107006},
year = {2020},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2019.107006},
url = {https://www.sciencedirect.com/science/article/pii/S001046551930342X},
author = {D. Willsch and M. Willsch and H. {De Raedt} and K. Michielsen},
keywords = {Support vector machine, Kernel-based SVM, Machine learning, Classification, Quantum computation, Quantum annealing},
abstract = {Kernel-based support vector machines (SVMs) are supervised machine learning algorithms for classification and regression problems. We introduce a method to train SVMs on a D-Wave 2000Q quantum annealer and study its performance in comparison to SVMs trained on conventional computers. The method is applied to both synthetic data and real data obtained from biology experiments. We find that the quantum annealer produces an ensemble of different solutions that often generalizes better to unseen data than the single global minimum of an SVM trained on a conventional computer, especially in cases where only limited training data is available. For cases with more training data than currently fits on the quantum annealer, we show that a combination of classifiers for subsets of the data almost always produces stronger joint classifiers than the conventional SVM for the same parameters.}
}
@article{ML_models_QUBO_Linear_SVM_k_Means,
	title = {{QUBO} {Formulations} for {Training} {Machine} {Learning} {Models}},
	volume = {11},
	issn = {2045-2322},
	url = {http://arxiv.org/abs/2008.02369},
	doi = {10.1038/s41598-021-89461-4},
	abstract = {Training machine learning models on classical computers is usually a time and compute intensive process. With Moore’s law coming to an end and ever increasing demand for large-scale data analysis using machine learning, we must leverage non-conventional computing paradigms like quantum computing to train machine learning models efﬁciently. Adiabatic quantum computers like the D-Wave 2000Q can approximately solve NP-hard optimization problems, such as the quadratic unconstrained binary optimization (QUBO), faster than classical computers. Since many machine learning problems are also NP-hard, we believe adiabatic quantum computers might be instrumental in training machine learning models efﬁciently in the post Moore’s law era. In order to solve a problem on adiabatic quantum computers, it must be formulated as a QUBO problem, which is a challenging task in itself. In this paper, we formulate the training problems of three machine learning models—linear regression, support vector machine (SVM) and equal-sized kmeans clustering—as QUBO problems so that they can be trained on adiabatic quantum computers efﬁciently. We also analyze the time and space complexities of our formulations and compare them to the state-of-the-art classical algorithms for training these machine learning models. We show that the time and space complexities of our formulations are better (in the case of SVM and equal-sized k-means clustering) or equivalent (in case of linear regression) to their classical counterparts.},
	language = {en},
	number = {1},
	urldate = {2024-05-29},
	journal = {Sci Rep},
	author = {Date, Prasanna and Arthur, Davis and Pusey-Nazzaro, Lauren},
	month = may,
	year = {2021},
	note = {arXiv:2008.02369 [physics, stat]},
	keywords = {62J05, 68T05, 68Q12, Computer Science - Machine Learning, I.2.6, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
	pages = {10029},
	file = {Date et al. - 2021 - QUBO Formulations for Training Machine Learning Mo.pdf:C\:\\Users\\kakic\\Zotero\\storage\\9DPTSMA2\\Date et al. - 2021 - QUBO Formulations for Training Machine Learning Mo.pdf:application/pdf},
}
@article {PMID:20421673,
	Title = {Transformation of general binary MRF minimization to the first-order case},
	Author = {Ishikawa, Hiroshi},
	DOI = {10.1109/tpami.2010.91},
	Number = {6},
	Volume = {33},
	Month = {June},
	Year = {2011},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	ISSN = {0162-8828},
	Pages = {1234—1249},
	Abstract = {We introduce a transformation of general higher-order Markov random field with binary labels into a first-order one that has the same minima as the original. Moreover, we formalize a framework for approximately minimizing higher-order multi-label MRF energies that combines the new reduction with the fusion-move and QPBO algorithms. While many computer vision problems today are formulated as energy minimization problems, they have mostly been limited to using first-order energies, which consist of unary and pairwise clique potentials, with a few exceptions that consider triples. This is because of the lack of efficient algorithms to optimize energies with higher-order interactions. Our algorithm challenges this restriction that limits the representational power of the models so that higher-order energies can be used to capture the rich statistics of natural scenes. We also show that some minimization methods can be considered special cases of the present framework, as well as comparing the new method experimentally with other such techniques.},
	URL = {http://www.f.waseda.jp/hfs/TGBMMTTFOC.pdf},
}
@INPROCEEDINGS{MRF_x_QUBO,
  author={Otgonbaatar, Soronzonbold and Datcu, Mihai},
  booktitle={IGARSS 2020 - 2020 IEEE International Geoscience and Remote Sensing Symposium}, 
  title={Quantum Annealing Approach: Feature Extraction and Segmentation of Synthetic Aperture Radar Image}, 
  year={2020},
  volume={},
  number={},
  pages={3692-3695},
  keywords={Feature extraction;Annealing;Quantum computing;Mathematical model;Computational modeling;Data mining;Optimization;texture segmentation;auto-binomial;d-wave;quantum annealing;QUBO},
  doi={10.1109/IGARSS39084.2020.9323504}}
@article{bayesian_networs_with_QUBO,
	title = {Bayesian {Network} {Structure} {Learning} {Using} {Quantum} {Annealing}},
	volume = {224},
	issn = {1951-6355, 1951-6401},
	url = {http://arxiv.org/abs/1407.3897},
	doi = {10.1140/epjst/e2015-02349-9},
	abstract = {We introduce a method for the problem of learning the structure of a Bayesian network using the quantum adiabatic algorithm. We do so by introducing an eﬃcient reformulation of a standard posteriorprobability scoring function on graphs as a pseudo-Boolean function, which is equivalent to a system of 2-body Ising spins, as well as suitable penalty terms for enforcing the constraints necessary for the reformulation; our proposed method requires O(n2) qubits for n Bayesian network variables. Furthermore, we prove lower bounds on the necessary weighting of these penalty terms. The logical structure resulting from the mapping has the appealing property that it is instance-independent for a given number of Bayesian network variables, as well as being independent of the number of data cases.},
	language = {en},
	number = {1},
	urldate = {2024-05-29},
	journal = {Eur. Phys. J. Spec. Top.},
	author = {O'Gorman, Bryan and Perdomo-Ortiz, Alejandro and Babbush, Ryan and Aspuru-Guzik, Alan and Smelyanskiy, Vadim},
	month = feb,
	year = {2015},
	note = {arXiv:1407.3897 [quant-ph]},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	pages = {163--188},
	file = {O'Gorman et al. - 2015 - Bayesian Network Structure Learning Using Quantum .pdf:C\:\\Users\\kakic\\Zotero\\storage\\RX3BDB6H\\O'Gorman et al. - 2015 - Bayesian Network Structure Learning Using Quantum .pdf:application/pdf},
}
@ARTICLE{HUBO_solving_algorithm,
  author={Norimoto, Masaya and Mori, Ryuhei and Ishikawa, Naoki},
  journal={IEEE Transactions on Communications}, 
  title={Quantum Algorithm for Higher-Order Unconstrained Binary Optimization and MIMO Maximum Likelihood Detection}, 
  year={2023},
  volume={71},
  number={4},
  pages={1926-1939},
  keywords={Qubit;Symbols;Quantum computing;Linear programming;Complexity theory;Optimization;MIMO communication;Grover adaptive search (GAS);quadratic unconstrained binary optimization (QUBO);higher-order unconstrained binary optimization (HUBO);multiple-input multiple-output (MIMO);maximum-likelihood detection (MLD)},
  doi={10.1109/TCOMM.2023.3244924}
}
@INPROCEEDINGS{PandaSet,
  author={Xiao, Pengchuan and Shao, Zhenlei and Hao, Steven and Zhang, Zishuo and Chai, Xiaolin and Jiao, Judy and Li, Zesong and Wu, Jian and Sun, Kai and Jiang, Kun and Wang, Yunlong and Yang, Diange},
  booktitle={2021 IEEE International Intelligent Transportation Systems Conference (ITSC)}, 
  title={PandaSet: Advanced Sensor Suite Dataset for Autonomous Driving}, 
  year={2021},
  volume={},
  number={},
  pages={3095-3101},
  keywords={Training;Measurement;Laser radar;Three-dimensional displays;Semantics;Object detection;Data collection},
  doi={10.1109/ITSC48978.2021.9565009}
}
@misc{PixSet,
title={PixSet : An Opportunity for 3D Computer Vision to Go Beyond Point Clouds With a Full-Waveform LiDAR Dataset},
author={Jean-Luc D�ziel and Pierre Merriaux and Francis Tremblay and Dave Lessard and Dominique Plourde and Julien Stanguennec and Pierre Goulet and Pierre Olivier},
year={2021},
eprint={2102.12010},
archivePrefix={arXiv},
primaryClass={cs.RO}
}
@INPROCEEDINGS {Cityscapes,
author = {M. Cordts and M. Omran and S. Ramos and T. Rehfeld and M. Enzweiler and R. Benenson and U. Franke and S. Roth and B. Schiele},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {The Cityscapes Dataset for Semantic Urban Scene Understanding},
year = {2016},
volume = {},
issn = {1063-6919},
pages = {3213-3223},
keywords = {urban areas;semantics;visualization;benchmark testing;vehicles;training;complexity theory},
doi = {10.1109/CVPR.2016.350},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.350},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}
@INPROCEEDINGS{BDD100K,
  author={Yu, Fisher and Chen, Haofeng and Wang, Xin and Xian, Wenqi and Chen, Yingying and Liu, Fangchen and Madhavan, Vashisht and Darrell, Trevor},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning}, 
  year={2020},
  volume={},
  number={},
  pages={2633-2642},
  keywords={Task analysis;Visualization;Roads;Image segmentation;Meteorology;Training;Benchmark testing},
  doi={10.1109/CVPR42600.2020.00271}}
@article{A2D2_Audi_dataset,

title={{A2D2: Audi Autonomous Driving Dataset}},

author={Jakob Geyer and Yohannes Kassahun and Mentar Mahmudi and Xavier Ricou and Rupesh Durgesh and Andrew S. Chung and Lorenz Hauswald and Viet Hoang Pham and Maximilian M{\"u}hlegg and Sebastian Dorn and Tiffany Fernandez and Martin J{\"a}nicke and Sudesh Mirashi and Chiragkumar Savani and Martin Sturm and Oleksandr Vorobiov and Martin Oelker and Sebastian Garreis and Peter Schuberth},

year={2020},

eprint={2004.06320},

archivePrefix={arXiv},

primaryClass={cs.CV},

url = {https://www.a2d2.audi}

}
@inproceedings{IDD_indian_dataset,
author = {Varma, Girish and Subramanian, Anbumani and Namboodiri, Anoop and Chandraker, Manmohan and Jawahar, C.V.},
year = {2019},
month = {01},
pages = {1743-1751},
title = {IDD: A Dataset for Exploring Problems of Autonomous Navigation in Unconstrained Environments},
doi = {10.1109/WACV.2019.00190}
}
@article{KAIST,
title = "KAIST Multi-Spectral Day/Night Data Set for Autonomous and Assisted Driving",
abstract = "We introduce the KAIST multi-spectral data set, which covers a great range of drivable regions, from urban to residential, for autonomous systems. Our data set provides the different perspectives of the world captured in coarse time slots (day and night), in addition to fine time slots (sunrise, morning, afternoon, sunset, night, and dawn). For all-day perception of autonomous systems, we propose the use of a different spectral sensor, i.e., a thermal imaging camera. Toward this goal, we develop a multi-sensor platform, which supports the use of a co-aligned RGB/Thermal camera, RGB stereo, 3-D LiDAR, and inertial sensors (GPS/IMU) and a related calibration technique. We design a wide range of visual perception tasks including the object detection, drivable region detection, localization, image enhancement, depth estimation, and colorization using a single/multi-spectral approach. In this paper, we provide a description of our benchmark with the recording platform, data format, development toolkits, and lessons about the progress of capturing data sets.",
keywords = "Dataset, KAIST multi-sepctral, advanced driver assistance system, autonomous driving, benchmarks, multi-spectral dataset in day and night, multi-spectral vehicle system",
author = "Yukyung Choi and Namil Kim and Soonmin Hwang and Kibaek Park and Yoon, {Jae Shin} and Kyounghwan An and Kweon, {In So}",
note = "Publisher Copyright: {\textcopyright} 2000-2011 IEEE.",
year = "2018",
month = mar,
doi = "10.1109/TITS.2018.2791533",
language = "English",
volume = "19",
pages = "934--948",
journal = "IEEE Transactions on Intelligent Transportation Systems",
issn = "1524-9050",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
number = "3",
}
@article{nuscenes2019,
 title={nuScenes: A multimodal dataset for autonomous driving},
  author={Holger Caesar and Varun Bankiti and Alex H. Lang and Sourabh Vora and 
          Venice Erin Liong and Qiang Xu and Anush Krishnan and Yu Pan and 
          Giancarlo Baldan and Oscar Beijbom}, 
  journal={arXiv preprint arXiv:1903.11027},
  year={2019}
}
@inproceedings{kitti-Geiger2012CVPR,
  author = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
  title = {Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2012}
}

@article{kitti-Luiten2020IJCV,
  author = {Jonathon Luiten and Aljosa Osep and Patrick Dendorfer and Philip Torr and Andreas Geiger and Laura Leal-Taixe and Bastian Leibe},
  title = {HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking},
  journal = {International Journal of Computer Vision (IJCV)},
  year = {2020}
}
@INPROCEEDINGS{Waymo_dataset,
  author={Sun, Pei and Kretzschmar, Henrik and Dotiwalla, Xerxes and Chouard, Aur�lien and Patnaik, Vijaysai and Tsui, Paul and Guo, James and Zhou, Yin and Chai, Yuning and Caine, Benjamin and Vasudevan, Vijay and Han, Wei and Ngiam, Jiquan and Zhao, Hang and Timofeev, Aleksei and Ettinger, Scott and Krivokon, Maxim and Gao, Amy and Joshi, Aditya and Zhang, Yu and Shlens, Jonathon and Chen, Zhifeng and Anguelov, Dragomir},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Scalability in Perception for Autonomous Driving: Waymo Open Dataset}, 
  year={2020},
  volume={},
  number={},
  pages={2443-2451},
  keywords={Laser radar;Cameras;Three-dimensional displays;Two dimensional displays;Autonomous vehicles;Radar tracking;Semantics},
  doi={10.1109/CVPR42600.2020.00252}}
@article{cruz-santos_qubo_2018,
	title = {A {QUBO} {Formulation} of the {Stereo} {Matching} {Problem} for {D}-{Wave} {Quantum} {Annealers}},
	volume = {20},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/20/10/786},
	doi = {10.3390/e20100786},
	abstract = {In this paper, we propose a methodology to solve the stereo matching problem through quantum annealing optimization. Our proposal takes advantage of the existing Min-Cut/Max-Flow network formulation of computer vision problems. Based on this network formulation, we construct a quadratic pseudo-Boolean function and then optimize it through the use of the D-Wave quantum annealing technology. Experimental validation using two kinds of stereo pair of images, random dot stereograms and gray-scale, shows that our methodology is effective.},
	language = {en},
	number = {10},
	urldate = {2024-02-25},
	journal = {Entropy},
	author = {Cruz-Santos, William and Venegas-Andraca, Salvador and Lanzagorta, Marco},
	month = oct,
	year = {2018},
	pages = {786},
	file = {Cruz-Santos et al. - 2018 - A QUBO Formulation of the Stereo Matching Problem .pdf:C\:\\Users\\kakic\\Zotero\\storage\\2ET2VEDM\\Cruz-Santos et al. - 2018 - A QUBO Formulation of the Stereo Matching Problem .pdf:application/pdf},
}

@article{bapst_pattern_2020,
	title = {A {Pattern} {Recognition} {Algorithm} for {Quantum} {Annealers}},
	volume = {4},
	issn = {2510-2036, 2510-2044},
	url = {http://link.springer.com/10.1007/s41781-019-0032-5},
	doi = {10.1007/s41781-019-0032-5},
	abstract = {The reconstruction of charged particles will be a key computing challenge for the high-luminosity Large Hadron Collider (HL-LHC) where increased data rates lead to a large increase in running time for current pattern recognition algorithms. An alternative approach explored here expresses pattern recognition as a quadratic unconstrained binary optimization (QUBO), which allows algorithms to be run on classical and quantum annealers. While the overall timing of the proposed approach and its scaling has still to be measured and studied, we demonstrate that, in terms of efficiency and purity, the same physics performance of the LHC tracking algorithms can be achieved. More research will be needed to achieve comparable performance in HL-LHC conditions, as increasing track density decreases the purity of the QUBO track segment classifier.},
	language = {en},
	number = {1},
	urldate = {2024-02-25},
	journal = {Comput Softw Big Sci},
	author = {Bapst, Frédéric and Bhimji, Wahid and Calafiura, Paolo and Gray, Heather and Lavrijsen, Wim and Linder, Lucy and Smith, Alex},
	month = dec,
	year = {2020},
	pages = {1},
	file = {Bapst et al. - 2020 - A Pattern Recognition Algorithm for Quantum Anneal.pdf:C\:\\Users\\kakic\\Zotero\\storage\\P6ECPLJC\\Bapst et al. - 2020 - A Pattern Recognition Algorithm for Quantum Anneal.pdf:application/pdf},
}

@article{zlokapa_charged_2021,
	title = {Charged particle tracking with quantum annealing optimization},
	volume = {3},
	issn = {2524-4906, 2524-4914},
	url = {https://link.springer.com/10.1007/s42484-021-00054-w},
	doi = {10.1007/s42484-021-00054-w},
	abstract = {At the High Luminosity Large Hadron Collider (HL-LHC), traditional track reconstruction techniques that are critical for physics analysis will need to be upgraded to scale with track density. Quantum annealing has shown promise in its ability to solve combinatorial optimization problems amidst an ongoing effort to establish evidence of a quantum speedup. As a step towards exploiting such potential speedup, we investigate a track reconstruction approach by adapting the existing geometric Denby-Peterson (Hopfield) network method to the quantum annealing framework for HL-LHC conditions. We develop additional techniques to embed the problem onto existing and near-term quantum annealing hardware. Results using simulated annealing and quantum annealing with the D-Wave 2X system on the TrackML open dataset are presented, demonstrating the successful application of a quantum annealing algorithm to the track reconstruction challenge. We find that combinatorial optimization problems can effectively reconstruct tracks, suggesting possible applications for fast hardware-specific implementations at the HL-LHC while leaving open the possibility of a quantum speedup for tracking.},
	language = {en},
	number = {2},
	urldate = {2024-02-25},
	journal = {Quantum Mach. Intell.},
	author = {Zlokapa, Alexander and Anand, Abhishek and Vlimant, Jean-Roch and Duarte, Javier M. and Job, Joshua and Lidar, Daniel and Spiropulu, Maria},
	month = dec,
	year = {2021},
	pages = {27},
	file = {Zlokapa et al. - 2021 - Charged particle tracking with quantum annealing o.pdf:C\:\\Users\\kakic\\Zotero\\storage\\PVWFH3WX\\Zlokapa et al. - 2021 - Charged particle tracking with quantum annealing o.pdf:application/pdf},
}

@inproceedings{yan_eu_2020,
	address = {Las Vegas, NV, USA},
	title = {{EU} {Long}-term {Dataset} with {Multiple} {Sensors} for {Autonomous} {Driving}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72816-212-6},
	url = {https://ieeexplore.ieee.org/document/9341406/},
	doi = {10.1109/IROS45743.2020.9341406},
	abstract = {The ﬁeld of autonomous driving has grown tremendously over the past few years, along with the rapid progress in sensor technology. One of the major purposes of using sensors is to provide environment perception for vehicle understanding, learning and reasoning, and ultimately interacting with the environment. In this paper, we ﬁrst introduce a multisensor platform allowing vehicle to perceive its surroundings and locate itself in a more efﬁcient and accurate way. The platform integrates eleven heterogeneous sensors including various cameras and lidars, a radar, an IMU (Inertial Measurement Unit), and a GPS-RTK (Global Positioning System / Real-Time Kinematic), while exploits a ROS (Robot Operating System) based software to process the sensory data. Then, we present a new dataset (https://epan-utbm.github.io/ utbm\_robocar\_dataset/) for autonomous driving captured many new research challenges (e.g. highly dynamic environment), and especially for long-term autonomy (e.g. creating and maintaining maps), collected with our instrumented vehicle, publicly available to the community.},
	language = {en},
	urldate = {2024-03-29},
	booktitle = {2020 {IEEE}/{RSJ} {International} {Conference} on {Intelligent} {Robots} and {Systems} ({IROS})},
	publisher = {IEEE},
	author = {Yan, Zhi and Sun, Li and Krajnik, Tomas and Ruichek, Yassine},
	month = oct,
	year = {2020},
	pages = {10697--10704},
	file = {Yan et al. - 2020 - EU Long-term Dataset with Multiple Sensors for Aut.pdf:C\:\\Users\\kakic\\Zotero\\storage\\7MPXTLX9\\Yan et al. - 2020 - EU Long-term Dataset with Multiple Sensors for Aut.pdf:application/pdf},
}

@inproceedings{koschorrek_multi-sensor_2013,
	address = {OR, USA},
	title = {A {Multi}-sensor {Traffic} {Scene} {Dataset} with {Omnidirectional} {Video}},
	isbn = {978-0-7695-4990-3},
	url = {http://ieeexplore.ieee.org/document/6595954/},
	doi = {10.1109/CVPRW.2013.110},
	abstract = {The development of vehicles that perceive their environment, in particular those using computer vision, indispensably requires large databases of sensor recordings obtained from real cars driven in realistic trafﬁc situations. These datasets should be time shaped for enabling synchronization of sensor data from different sources. Furthermore, full surround environment perception requires high frame rates of synchronized omnidirectional video data to prevent information loss at any speeds.},
	language = {en},
	urldate = {2024-03-29},
	booktitle = {2013 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops}},
	publisher = {IEEE},
	author = {Koschorrek, Philipp and Piccini, Tommaso and Oberg, Per and Felsberg, Michael and Nielsen, Lars and Mester, Rudolf},
	month = jun,
	year = {2013},
	pages = {727--734},
	file = {Koschorrek et al. - 2013 - A Multi-sensor Traffic Scene Dataset with Omnidire.pdf:C\:\\Users\\kakic\\Zotero\\storage\\GRBBNQRT\\Koschorrek et al. - 2013 - A Multi-sensor Traffic Scene Dataset with Omnidire.pdf:application/pdf},
}

@misc{brodermann_muses_2024,
	title = {{MUSES}: {The} {Multi}-{Sensor} {Semantic} {Perception} {Dataset} for {Driving} under {Uncertainty}},
	shorttitle = {{MUSES}},
	url = {http://arxiv.org/abs/2401.12761},
	abstract = {Achieving level-5 driving automation in autonomous vehicles necessitates a robust semantic visual perception system capable of parsing data from different sensors across diverse conditions. However, existing semantic perception datasets often lack important non-camera modalities typically used in autonomous vehicles, or they do not exploit such modalities to aid and improve semantic annotations in challenging conditions. To address this, we introduce MUSES, the MUlti-SEnsor Semantic perception dataset for driving in adverse conditions under increased uncertainty. MUSES includes synchronized multimodal recordings with 2D panoptic annotations for 2500 images captured under diverse weather and illumination. The dataset integrates a frame camera, a lidar, a radar, an event camera, and an IMU/GNSS sensor. Our new two-stage panoptic annotation protocol captures both class-level and instance-level uncertainty in the ground truth and enables the novel task of uncertainty-aware panoptic segmentation we introduce, along with standard semantic and panoptic segmentation. MUSES proves both effective for training and challenging for evaluating models under diverse visual conditions, and it opens new avenues for research in multimodal and uncertainty-aware dense semantic perception. Our dataset and benchmark will be made publicly available.},
	language = {en},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Brödermann, Tim and Bruggemann, David and Sakaridis, Christos and Ta, Kevin and Liagouris, Odysseas and Corkill, Jason and Van Gool, Luc},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12761 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Brödermann et al. - 2024 - MUSES The Multi-Sensor Semantic Perception Datase.pdf:C\:\\Users\\kakic\\Zotero\\storage\\TPTW63NM\\Brödermann et al. - 2024 - MUSES The Multi-Sensor Semantic Perception Datase.pdf:application/pdf},
}

@article{trybala_min3d_2023,
	title = {{MIN3D} {Dataset}: {MultI}-{seNsor} {3D} {Mapping} with an {Unmanned} {Ground} {Vehicle}},
	volume = {91},
	issn = {2512-2789, 2512-2819},
	shorttitle = {{MIN3D} {Dataset}},
	url = {https://link.springer.com/10.1007/s41064-023-00260-0},
	doi = {10.1007/s41064-023-00260-0},
	abstract = {The research potential in the field of mobile mapping technologies is often hindered by several constraints. These include the need for costly hardware to collect data, limited access to target sites with specific environmental conditions or the collection of ground truth data for a quantitative evaluation of the developed solutions. To address these challenges, the research community has often prepared open datasets suitable for developments and testing. However, the availability of datasets that encompass truly demanding mixed indoor–outdoor and subterranean conditions, acquired with diverse but synchronized sensors, is currently limited. To alleviate this issue, we propose the MIN3D dataset (MultI-seNsor 3D mapping with an unmanned ground vehicle for mining applications) which includes data gathered using a wheeled mobile robot in two distinct locations: (i) textureless dark corridors and outside parts of a university campus and (ii) tunnels of an underground WW2 site in Walim (Poland). MIN3D comprises around 150 GB of raw data, including images captured by multiple co-calibrated monocular, stereo and thermal cameras, two LiDAR sensors and three inertial measurement units. Reliable ground truth (GT) point clouds were collected using a survey-grade terrestrial laser scanner. By openly sharing this dataset, we aim to support the efforts of the scientific community in developing robust methods for navigation and mapping in challenging underground conditions. In the paper, we describe the collected data and provide an initial accuracy assessment of some visual- and LiDAR-based simultaneous localization and mapping (SLAM) algorithms for selected sequences. Encountered problems, open research questions and areas that could benefit from utilizing our dataset are discussed. Data are available at https://3dom.fbk.eu/benchmarks.},
	language = {en},
	number = {6},
	urldate = {2024-03-29},
	journal = {PFG},
	author = {Trybała, Paweł and Szrek, Jarosław and Remondino, Fabio and Kujawa, Paulina and Wodecki, Jacek and Blachowski, Jan and Zimroz, Radosław},
	month = dec,
	year = {2023},
	pages = {425--442},
	file = {Trybała et al. - 2023 - MIN3D Dataset MultI-seNsor 3D Mapping with an Unm.pdf:C\:\\Users\\kakic\\Zotero\\storage\\QNHWWPPZ\\Trybała et al. - 2023 - MIN3D Dataset MultI-seNsor 3D Mapping with an Unm.pdf:application/pdf},
}

@misc{ma_holovic_2024,
	title = {{HoloVIC}: {Large}-scale {Dataset} and {Benchmark} for {Multi}-{Sensor} {Holographic} {Intersection} and {Vehicle}-{Infrastructure} {Cooperative}},
	shorttitle = {{HoloVIC}},
	url = {http://arxiv.org/abs/2403.02640},
	abstract = {Vehicle-to-everything (V2X) is a popular topic in the field of Autonomous Driving in recent years. Vehicleinfrastructure cooperation (VIC) becomes one of the important research area. Due to the complexity of traffic conditions such as blind spots and occlusion, it greatly limits the perception capabilities of single-view roadside sensing systems. To further enhance the accuracy of roadside perception and provide better information to the vehicle side, in this paper, we constructed holographic intersections with various layouts to build a large-scale multi-sensor holographic vehicle-infrastructure cooperation dataset, called HoloVIC. Our dataset includes 3 different types of sensors (Camera, Lidar, Fisheye) and employs 4 sensor-layouts based on the different intersections. Each intersection is equipped with 6-18 sensors to capture synchronous data. While autonomous vehicles pass through these intersections for collecting VIC data. HoloVIC contains in total on 100k+ synchronous frames from different sensors. Additionally, we annotated 3D bounding boxes based on Camera, Fisheye, and Lidar. We also associate the IDs of the same objects across different devices and consecutive frames in sequence. Based on HoloVIC, we formulated four tasks to facilitate the development of related research. We also provide benchmarks for these tasks.},
	language = {en},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Ma, Cong and Qiao, Lei and Zhu, Chengkai and Liu, Kai and Kong, Zelong and Li, Qing and Zhou, Xueqi and Kan, Yuheng and Wu, Wei},
	month = mar,
	year = {2024},
	note = {arXiv:2403.02640 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ma et al. - 2024 - HoloVIC Large-scale Dataset and Benchmark for Mul.pdf:C\:\\Users\\kakic\\Zotero\\storage\\5A8QZLIH\\Ma et al. - 2024 - HoloVIC Large-scale Dataset and Benchmark for Mul.pdf:application/pdf},
}

@misc{gao_memotr_2024,
	title = {{MeMOTR}: {Long}-{Term} {Memory}-{Augmented} {Transformer} for {Multi}-{Object} {Tracking}},
	shorttitle = {{MeMOTR}},
	url = {http://arxiv.org/abs/2307.15700},
	abstract = {As a video task, Multi-Object Tracking (MOT) is expected to capture temporal information of targets effectively. Unfortunately, most existing methods only explicitly exploit the object features between adjacent frames, while lacking the capacity to model long-term temporal information. In this paper, we propose MeMOTR, a long-term memory-augmented Transformer for multi-object tracking. Our method is able to make the same object’s track embedding more stable and distinguishable by leveraging longterm memory injection with a customized memory-attention layer. This significantly improves the target association ability of our model. Experimental results on DanceTrack show that MeMOTR impressively surpasses the state-of-theart method by 7.9\% and 13.0\% on HOTA and AssA metrics, respectively. Furthermore, our model also outperforms other Transformer-based methods on association performance on MOT17 and generalizes well on BDD100K. Code is available at https://github.com/MCG-NJU/MeMOTR.},
	language = {en},
	urldate = {2024-03-29},
	publisher = {arXiv},
	author = {Gao, Ruopeng and Wang, Limin},
	month = feb,
	year = {2024},
	note = {arXiv:2307.15700 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Gao i Wang - 2024 - MeMOTR Long-Term Memory-Augmented Transformer for.pdf:C\:\\Users\\kakic\\Zotero\\storage\\49A8CYIN\\Gao i Wang - 2024 - MeMOTR Long-Term Memory-Augmented Transformer for.pdf:application/pdf},
}
